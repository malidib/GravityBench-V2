[2025-08-10 23:53:10] Scenario: roche_lobe_radius - Variation: 3.1 M, 0.18 M Elliptical - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:11] Scenario: roche_lobe_radius - Variation: 3.1 M, 0.18 M Elliptical - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:12] Scenario: roche_lobe_radius - Variation: 3.1 M, 0.18 M Elliptical - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:15] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:15] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:16] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:18] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 2.03 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:19] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 2.03 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:19] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 2.03 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:22] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:22] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:22] Scenario: modified_gravity_power_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:35] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 1.7e9 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:35] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 1.7e9 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:36] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 1.7e9 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:45] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:45] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:47] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:57] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:57] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:53:57] Scenario: linear_drag - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:00] Scenario: time_fraction_acceleraton_below_mean - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:00] Scenario: time_fraction_acceleraton_below_mean - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:01] Scenario: time_fraction_acceleraton_below_mean - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:04] Scenario: time_fraction_acceleraton_below_mean - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:04] Scenario: time_fraction_acceleraton_below_mean - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:04] Scenario: time_fraction_acceleraton_below_mean - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:07] Scenario: time_fraction_acceleraton_below_mean - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:07] Scenario: time_fraction_acceleraton_below_mean - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:08] Scenario: time_fraction_acceleraton_below_mean - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:08] Scenario: apoastron - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:08] Scenario: apoastron - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:09] Scenario: apoastron - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:09] Scenario: apoastron - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:09] Scenario: apoastron - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:09] Scenario: apoastron - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:10] Scenario: apoastron - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:11] Scenario: apoastron - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:11] Scenario: apoastron - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:18] Scenario: apoastron - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:18] Scenario: apoastron - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:18] Scenario: apoastron - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:36] Scenario: apoastron - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:36] Scenario: apoastron - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:36] Scenario: apoastron - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:37] Scenario: area_swept_over_time_apo - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:37] Scenario: area_swept_over_time_apo - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:37] Scenario: area_swept_over_time_apo - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:38] Scenario: area_swept_over_time_apo - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:38] Scenario: area_swept_over_time_apo - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:38] Scenario: area_swept_over_time_apo - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:39] Scenario: area_swept_over_time_apo - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:39] Scenario: area_swept_over_time_apo - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:39] Scenario: area_swept_over_time_apo - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:39] Scenario: area_swept_over_time_peri - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:40] Scenario: area_swept_over_time_peri - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:40] Scenario: area_swept_over_time_peri - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:40] Scenario: area_swept_over_time_peri - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:40] Scenario: area_swept_over_time_peri - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:41] Scenario: area_swept_over_time_peri - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:41] Scenario: area_swept_over_time_peri - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:41] Scenario: area_swept_over_time_peri - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:41] Scenario: area_swept_over_time_peri - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:42] Scenario: avg_distance_COM_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:42] Scenario: avg_distance_COM_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:42] Scenario: avg_distance_COM_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:43] Scenario: avg_distance_COM_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:43] Scenario: avg_distance_COM_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:43] Scenario: avg_distance_COM_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:43] Scenario: avg_distance_COM_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:44] Scenario: avg_distance_COM_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:44] Scenario: avg_distance_COM_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:44] Scenario: avg_distance_COM_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:44] Scenario: avg_distance_COM_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:45] Scenario: avg_distance_COM_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:45] Scenario: avg_distance_COM_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:45] Scenario: avg_distance_COM_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:45] Scenario: avg_distance_COM_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:46] Scenario: avg_distance_COM_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:46] Scenario: avg_distance_COM_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:46] Scenario: avg_distance_COM_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:46] Scenario: eccentricity - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:47] Scenario: eccentricity - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:47] Scenario: eccentricity - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:47] Scenario: eccentricity - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:48] Scenario: eccentricity - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:48] Scenario: eccentricity - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:48] Scenario: eccentricity - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:48] Scenario: eccentricity - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:49] Scenario: eccentricity - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:49] Scenario: eccentricity - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:49] Scenario: eccentricity - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:49] Scenario: eccentricity - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:50] Scenario: eccentricity - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:50] Scenario: eccentricity - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:50] Scenario: eccentricity - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:51] Scenario: eccentricity - Variation: 3.1 M, 0.18 M Elliptical - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:51] Scenario: eccentricity - Variation: 3.1 M, 0.18 M Elliptical - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:52] Scenario: eccentricity - Variation: 3.1 M, 0.18 M Elliptical - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:52] Scenario: kepler_3rd_law - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:52] Scenario: kepler_3rd_law - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:52] Scenario: kepler_3rd_law - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:53] Scenario: kepler_3rd_law - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:53] Scenario: kepler_3rd_law - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:53] Scenario: kepler_3rd_law - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:53] Scenario: kepler_3rd_law - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:54] Scenario: kepler_3rd_law - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:54] Scenario: kepler_3rd_law - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:54] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:54] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:54] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:55] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 2.03 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:55] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 2.03 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:55] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 2.03 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:56] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:56] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:56] Scenario: kepler_3rd_law - Variation: 10.1M, 5.6 M, Modified Gravity 1.97 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:57] Scenario: mass_largest_star - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:57] Scenario: mass_largest_star - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:57] Scenario: mass_largest_star - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:58] Scenario: mass_largest_star - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:58] Scenario: mass_largest_star - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:58] Scenario: mass_largest_star - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:58] Scenario: mass_largest_star - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:59] Scenario: mass_largest_star - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:59] Scenario: mass_largest_star - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:59] Scenario: max_acceleration_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:54:59] Scenario: max_acceleration_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:00] Scenario: max_acceleration_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:00] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:00] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:01] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:01] Scenario: max_acceleration_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:01] Scenario: max_acceleration_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:01] Scenario: max_acceleration_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:02] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:03] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:03] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:04] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:04] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:04] Scenario: max_acceleration_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:05] Scenario: max_acceleration_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:05] Scenario: max_acceleration_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:05] Scenario: max_acceleration_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:06] Scenario: max_acceleration_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:06] Scenario: max_acceleration_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:06] Scenario: max_acceleration_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:06] Scenario: max_acceleration_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:07] Scenario: max_acceleration_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:07] Scenario: max_acceleration_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:07] Scenario: max_angular_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:07] Scenario: max_angular_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:08] Scenario: max_angular_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:08] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:08] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:09] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:09] Scenario: max_angular_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:09] Scenario: max_angular_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:10] Scenario: max_angular_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:10] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:10] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:11] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:11] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:11] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:12] Scenario: max_angular_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:12] Scenario: max_angular_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:12] Scenario: max_angular_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:13] Scenario: max_angular_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:13] Scenario: max_angular_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:13] Scenario: max_angular_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:14] Scenario: max_angular_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:14] Scenario: max_angular_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:14] Scenario: max_angular_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:14] Scenario: max_angular_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:15] Scenario: max_momentum_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:15] Scenario: max_momentum_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:15] Scenario: max_momentum_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:15] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:16] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:16] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:17] Scenario: max_momentum_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:17] Scenario: max_momentum_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:17] Scenario: max_momentum_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:18] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:18] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:18] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:19] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:19] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:19] Scenario: max_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:20] Scenario: max_momentum_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:20] Scenario: max_momentum_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:20] Scenario: max_momentum_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:20] Scenario: max_momentum_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:21] Scenario: max_momentum_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:21] Scenario: max_momentum_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:21] Scenario: max_momentum_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:21] Scenario: max_momentum_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:22] Scenario: max_momentum_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:22] Scenario: max_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:22] Scenario: max_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:22] Scenario: max_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:23] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:23] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:23] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:24] Scenario: max_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:24] Scenario: max_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:24] Scenario: max_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:25] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:25] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:25] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:26] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:26] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:26] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:29] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:29] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:30] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:33] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:33] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:33] Scenario: max_velocity_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:33] Scenario: max_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:34] Scenario: max_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:34] Scenario: max_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:34] Scenario: max_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:35] Scenario: max_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:35] Scenario: max_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:35] Scenario: max_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:36] Scenario: max_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:36] Scenario: max_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:36] Scenario: min_acceleration_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:36] Scenario: min_acceleration_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:37] Scenario: min_acceleration_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:37] Scenario: min_acceleration_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:37] Scenario: min_acceleration_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:37] Scenario: min_acceleration_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:38] Scenario: min_acceleration_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:38] Scenario: min_acceleration_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:38] Scenario: min_acceleration_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:38] Scenario: min_acceleration_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:39] Scenario: min_acceleration_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:39] Scenario: min_acceleration_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:39] Scenario: min_acceleration_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:39] Scenario: min_acceleration_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:40] Scenario: min_acceleration_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:40] Scenario: min_acceleration_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:40] Scenario: min_acceleration_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:40] Scenario: min_acceleration_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:41] Scenario: min_angular_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:41] Scenario: min_angular_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:41] Scenario: min_angular_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:42] Scenario: min_angular_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:42] Scenario: min_angular_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:42] Scenario: min_angular_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:42] Scenario: min_angular_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:43] Scenario: min_angular_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:43] Scenario: min_angular_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:44] Scenario: min_angular_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:44] Scenario: min_angular_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:45] Scenario: min_angular_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:45] Scenario: min_angular_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:45] Scenario: min_angular_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:45] Scenario: min_angular_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:46] Scenario: min_angular_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:46] Scenario: min_angular_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:46] Scenario: min_angular_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:47] Scenario: min_momentum_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:47] Scenario: min_momentum_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:47] Scenario: min_momentum_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:47] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:48] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:48] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:49] Scenario: min_momentum_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:49] Scenario: min_momentum_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:49] Scenario: min_momentum_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:49] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:50] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:50] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:50] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:51] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:51] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:51] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:51] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:52] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:52] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:52] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:52] Scenario: min_momentum_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:53] Scenario: min_momentum_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:53] Scenario: min_momentum_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:53] Scenario: min_momentum_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:54] Scenario: min_momentum_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:54] Scenario: min_momentum_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:54] Scenario: min_momentum_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:54] Scenario: min_momentum_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:55] Scenario: min_momentum_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:55] Scenario: min_momentum_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:55] Scenario: min_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:55] Scenario: min_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:56] Scenario: min_velocity_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:56] Scenario: min_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:56] Scenario: min_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:57] Scenario: min_velocity_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:57] Scenario: min_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:57] Scenario: min_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:58] Scenario: min_velocity_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:58] Scenario: min_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:58] Scenario: min_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:58] Scenario: min_velocity_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:59] Scenario: min_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:59] Scenario: min_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:55:59] Scenario: min_velocity_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:00] Scenario: min_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:00] Scenario: min_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:00] Scenario: min_velocity_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:00] Scenario: multiply_mass_period - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:01] Scenario: multiply_mass_period - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:01] Scenario: multiply_mass_period - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:01] Scenario: multiply_mass_period - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:01] Scenario: multiply_mass_period - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:02] Scenario: multiply_mass_period - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:02] Scenario: multiply_mass_period - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:02] Scenario: multiply_mass_period - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:03] Scenario: multiply_mass_period - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:03] Scenario: orbital_area_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:03] Scenario: orbital_area_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:04] Scenario: orbital_area_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:04] Scenario: orbital_area_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:04] Scenario: orbital_area_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:04] Scenario: orbital_area_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:05] Scenario: orbital_area_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:05] Scenario: orbital_area_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:05] Scenario: orbital_area_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:05] Scenario: orbital_area_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:06] Scenario: orbital_area_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:06] Scenario: orbital_area_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:06] Scenario: orbital_area_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:07] Scenario: orbital_area_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:07] Scenario: orbital_area_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:07] Scenario: orbital_area_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:08] Scenario: orbital_area_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:08] Scenario: orbital_area_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:08] Scenario: periastron - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:08] Scenario: periastron - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:09] Scenario: periastron - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:09] Scenario: periastron - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:09] Scenario: periastron - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:10] Scenario: periastron - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:10] Scenario: periastron - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:10] Scenario: periastron - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:11] Scenario: periastron - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:14] Scenario: periastron - Variation: 3.1 M, 0.18 M, Elliptical, Single Orbit - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:14] Scenario: periastron - Variation: 3.1 M, 0.18 M, Elliptical, Single Orbit - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:14] Scenario: periastron - Variation: 3.1 M, 0.18 M, Elliptical, Single Orbit - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:14] Scenario: period - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:15] Scenario: period - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:15] Scenario: period - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:15] Scenario: period - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:15] Scenario: period - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:16] Scenario: period - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:16] Scenario: period - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:16] Scenario: period - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:17] Scenario: period - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:17] Scenario: period - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:17] Scenario: period - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:18] Scenario: period - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:18] Scenario: period - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:18] Scenario: period - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:19] Scenario: period - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:19] Scenario: period - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:20] Scenario: period - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:20] Scenario: period - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:20] Scenario: period - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:20] Scenario: period - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:21] Scenario: period - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:21] Scenario: reduced_mass - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:21] Scenario: reduced_mass - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:21] Scenario: reduced_mass - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:22] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:22] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:22] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:23] Scenario: reduced_mass - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:23] Scenario: reduced_mass - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:23] Scenario: reduced_mass - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:24] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:24] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:24] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:25] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:25] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:25] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:26] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:26] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:26] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:27] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:27] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:28] Scenario: reduced_mass - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:28] Scenario: semi_major_axis - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:28] Scenario: semi_major_axis - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:29] Scenario: semi_major_axis - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:29] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:29] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:29] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:30] Scenario: semi_major_axis - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:30] Scenario: semi_major_axis - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:30] Scenario: semi_major_axis - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:31] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:31] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:31] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:32] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:32] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:32] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:32] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:33] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:33] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:33] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:34] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:34] Scenario: semi_major_axis - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:34] Scenario: semi_major_axis_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:35] Scenario: semi_major_axis_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:35] Scenario: semi_major_axis_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:36] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:36] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:36] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:37] Scenario: semi_major_axis_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:38] Scenario: semi_major_axis_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:38] Scenario: semi_major_axis_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:38] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:39] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:39] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:40] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:40] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:41] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:41] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:41] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:42] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:43] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:43] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:44] Scenario: semi_major_axis_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:44] Scenario: semi_major_axis_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:44] Scenario: semi_major_axis_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:45] Scenario: semi_major_axis_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:45] Scenario: semi_major_axis_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:46] Scenario: semi_major_axis_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:46] Scenario: semi_major_axis_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:46] Scenario: semi_major_axis_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:47] Scenario: semi_major_axis_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:47] Scenario: semi_major_axis_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:48] Scenario: semi_minor_axis - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:49] Scenario: semi_minor_axis - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:49] Scenario: semi_minor_axis - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:49] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:50] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:50] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:51] Scenario: semi_minor_axis - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:51] Scenario: semi_minor_axis - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:52] Scenario: semi_minor_axis - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:53] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:53] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:53] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:54] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:54] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:55] Scenario: semi_minor_axis - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:55] Scenario: semi_minor_axis_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:56] Scenario: semi_minor_axis_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:56] Scenario: semi_minor_axis_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:57] Scenario: semi_minor_axis_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:57] Scenario: semi_minor_axis_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:57] Scenario: semi_minor_axis_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:58] Scenario: semi_minor_axis_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:58] Scenario: semi_minor_axis_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:59] Scenario: semi_minor_axis_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:56:59] Scenario: semi_minor_axis_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:00] Scenario: semi_minor_axis_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:00] Scenario: semi_minor_axis_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:01] Scenario: semi_minor_axis_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:01] Scenario: semi_minor_axis_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:01] Scenario: semi_minor_axis_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:02] Scenario: semi_minor_axis_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:02] Scenario: semi_minor_axis_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:03] Scenario: semi_minor_axis_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:03] Scenario: specific_angular_momentum - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:04] Scenario: specific_angular_momentum - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:04] Scenario: specific_angular_momentum - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:05] Scenario: specific_angular_momentum - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:05] Scenario: specific_angular_momentum - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:06] Scenario: specific_angular_momentum - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:06] Scenario: specific_angular_momentum - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:07] Scenario: specific_angular_momentum - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:07] Scenario: specific_angular_momentum - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:08] Scenario: K+U - Variation: 10.1 M, 5.6 M, Unbound - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:08] Scenario: K+U - Variation: 10.1 M, 5.6 M, Unbound - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:09] Scenario: K+U - Variation: 10.1 M, 5.6 M, Unbound - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:09] Scenario: K+U - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:09] Scenario: K+U - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:10] Scenario: K+U - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:11] Scenario: K+U - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:11] Scenario: K+U - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:12] Scenario: K+U - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:12] Scenario: K+U - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:12] Scenario: K+U - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:13] Scenario: K+U - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:13] Scenario: K+U - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:14] Scenario: K+U - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:14] Scenario: K+U - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:15] Scenario: K+U - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:15] Scenario: K+U - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:16] Scenario: K+U - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:16] Scenario: total_mass - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:17] Scenario: total_mass - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:17] Scenario: total_mass - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:18] Scenario: total_mass - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:18] Scenario: total_mass - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:19] Scenario: total_mass - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:19] Scenario: total_mass - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:20] Scenario: total_mass - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:20] Scenario: total_mass - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:20] Scenario: mass_ratio - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:21] Scenario: mass_ratio - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:21] Scenario: mass_ratio - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:22] Scenario: mass_ratio - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:22] Scenario: mass_ratio - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:23] Scenario: mass_ratio - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:23] Scenario: mass_ratio - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:24] Scenario: mass_ratio - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:24] Scenario: mass_ratio - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:25] Scenario: virial_theorem - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:25] Scenario: virial_theorem - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:26] Scenario: virial_theorem - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:26] Scenario: virial_theorem - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:27] Scenario: virial_theorem - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:27] Scenario: virial_theorem - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:28] Scenario: virial_theorem - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:28] Scenario: virial_theorem - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:29] Scenario: virial_theorem - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:30] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 1.7e9 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:30] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 1.7e9 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:30] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 1.7e9 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:31] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:31] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:32] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:32] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:32] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:33] Scenario: virial_theorem - Variation: 7.7 M, 4.9 M, Drag tau = 8.3e8 Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:34] Scenario: travel_time_orbital_20per_path - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:34] Scenario: travel_time_orbital_20per_path - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:35] Scenario: travel_time_orbital_20per_path - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:35] Scenario: travel_time_orbital_20per_path - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:36] Scenario: travel_time_orbital_20per_path - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:36] Scenario: travel_time_orbital_20per_path - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:37] Scenario: travel_time_orbital_20per_path - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:37] Scenario: travel_time_orbital_20per_path - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:38] Scenario: travel_time_orbital_20per_path - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:38] Scenario: travel_time_orbital_70per_path - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:39] Scenario: travel_time_orbital_70per_path - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:39] Scenario: travel_time_orbital_70per_path - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:40] Scenario: travel_time_orbital_70per_path - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:40] Scenario: travel_time_orbital_70per_path - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:41] Scenario: travel_time_orbital_70per_path - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:41] Scenario: travel_time_orbital_70per_path - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:42] Scenario: travel_time_orbital_70per_path - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:43] Scenario: travel_time_orbital_70per_path - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:43] Scenario: is_bound - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:43] Scenario: is_bound - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:44] Scenario: is_bound - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:44] Scenario: is_bound - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:45] Scenario: is_bound - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:45] Scenario: is_bound - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:46] Scenario: is_bound - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:46] Scenario: is_bound - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:47] Scenario: is_bound - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:47] Scenario: is_bound - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:47] Scenario: is_bound - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:48] Scenario: is_bound - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:49] Scenario: is_bound - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:50] Scenario: is_bound - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:50] Scenario: is_bound - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:50] Scenario: is_bound - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:50] Scenario: is_bound - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:51] Scenario: is_bound - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:51] Scenario: is_bound - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:51] Scenario: is_bound - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:52] Scenario: is_bound - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:52] Scenario: is_bound - Variation: 10.1 M, 5.6 M, Unbound - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:53] Scenario: is_bound - Variation: 10.1 M, 5.6 M, Unbound - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:53] Scenario: is_bound - Variation: 10.1 M, 5.6 M, Unbound - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:54] Scenario: mass_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:54] Scenario: mass_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:55] Scenario: mass_star1 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:55] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:56] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:56] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:56] Scenario: mass_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:57] Scenario: mass_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:57] Scenario: mass_star1 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:58] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:58] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:59] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:57:59] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:00] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:00] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:01] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:01] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:02] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, yrAUMsun - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:02] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:02] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:03] Scenario: mass_star1 - Variation: 9.6 M, 3.1 M, cgs - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:03] Scenario: mass_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:04] Scenario: mass_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:04] Scenario: mass_star2 - Variation: 21.3 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:05] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:05] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:06] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:06] Scenario: mass_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:06] Scenario: mass_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:07] Scenario: mass_star2 - Variation: 0.18 M, 0.63 M - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:07] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:08] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:08] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M, Proper Motion - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:09] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:09] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

[2025-08-10 23:58:10] Scenario: mass_star2 - Variation: 9.6 M, 3.1 M, Proper Motion2 - Error: Error occurred: BadRequestError - Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}} - Traceback: Traceback (most recent call last):
  File "/Users/jaredlim/Downloads/GravityBench-V2/agents/tabular_agent.py", line 220, in run
    response = llm.chat.completions.create(
        model=self.model,
    ...<2 lines>...
        temperature=self.temperature
    )
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1087, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1256, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaredlim/Downloads/GravityBench-V2/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}

